# Neural Network Architecture Diagram

## Bidirectional GRU Model for Taglish Sentiment Classification

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INPUT LAYER                                  â”‚
â”‚                                                                      â”‚
â”‚              Raw Taglish Product Review Text                         â”‚
â”‚         "Maganda quality pero medyo mahal, sulit naman"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREPROCESSING PIPELINE                            â”‚
â”‚                                                                      â”‚
â”‚  1. Lowercasing                                                      â”‚
â”‚  2. Emoji Removal (regex pattern matching)                          â”‚
â”‚  3. Repeated Character Normalization (aaa â†’ aa)                      â”‚
â”‚  4. URL & Special Character Cleaning                                â”‚
â”‚  5. Stopword Removal (Tagalog + English combined)                   â”‚
â”‚  6. Tokenization (Keras Tokenizer, vocab=10,000)                    â”‚
â”‚  7. Padding/Truncating (max_length=37)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EMBEDDING LAYER                                 â”‚
â”‚                                                                      â”‚
â”‚  Input Shape:  (batch_size, 37)                                     â”‚
â”‚  Output Shape: (batch_size, 37, 128)                                â”‚
â”‚                                                                      â”‚
â”‚  - Vocabulary Size: 10,000 words                                    â”‚
â”‚  - Embedding Dimension: 128                                         â”‚
â”‚  - Trainable: Yes                                                   â”‚
â”‚  - Parameters: 1,280,000                                            â”‚
â”‚                                                                      â”‚
â”‚  Converts word indices â†’ dense vector representations                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BIDIRECTIONAL GRU LAYER                             â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Forward GRU (32)    â”‚  â”‚  Backward GRU (32)   â”‚                â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                â”‚
â”‚  â”‚  Processes left-to-  â”‚  â”‚  Processes right-to- â”‚                â”‚
â”‚  â”‚  right sequence      â”‚  â”‚  left sequence       â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚               â†“                        â†“                             â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                           â†“                                          â”‚
â”‚                  Concatenate Outputs                                 â”‚
â”‚                                                                      â”‚
â”‚  Input Shape:  (batch_size, 37, 128)                                â”‚
â”‚  Output Shape: (batch_size, 64)                                     â”‚
â”‚                                                                      â”‚
â”‚  - GRU Units: 32 per direction (64 total)                           â”‚
â”‚  - Dropout: 0.5                                                     â”‚
â”‚  - Recurrent Dropout: 0.3                                           â”‚
â”‚  - Return Sequences: False (only last timestep)                     â”‚
â”‚  - Activation: tanh (default)                                       â”‚
â”‚  - Parameters: 110,208                                              â”‚
â”‚                                                                      â”‚
â”‚  Captures sequential context from both directions                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DROPOUT LAYER #1                              â”‚
â”‚                                                                      â”‚
â”‚  - Rate: 0.5 (50% of neurons randomly dropped)                      â”‚
â”‚  - Prevents overfitting                                             â”‚
â”‚  - Applied during training only                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DENSE LAYER (Hidden)                             â”‚
â”‚                                                                      â”‚
â”‚  Input Shape:  (batch_size, 64)                                     â”‚
â”‚  Output Shape: (batch_size, 64)                                     â”‚
â”‚                                                                      â”‚
â”‚  - Units: 64                                                        â”‚
â”‚  - Activation: ReLU (Rectified Linear Unit)                         â”‚
â”‚  - Kernel Regularization: L2 (Î»=1e-4)                               â”‚
â”‚  - Parameters: 4,160                                                â”‚
â”‚                                                                      â”‚
â”‚  Learns higher-level abstract features                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DROPOUT LAYER #2                              â”‚
â”‚                                                                      â”‚
â”‚  - Rate: 0.5 (50% of neurons randomly dropped)                      â”‚
â”‚  - Additional regularization before output                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OUTPUT LAYER (Dense)                             â”‚
â”‚                                                                      â”‚
â”‚  Input Shape:  (batch_size, 64)                                     â”‚
â”‚  Output Shape: (batch_size, 5)                                      â”‚
â”‚                                                                      â”‚
â”‚  - Units: 5 (one per star rating)                                   â”‚
â”‚  - Activation: Softmax                                              â”‚
â”‚  - Parameters: 325                                                  â”‚
â”‚                                                                      â”‚
â”‚  Produces probability distribution over 5 classes:                   â”‚
â”‚  [P(1-star), P(2-star), P(3-star), P(4-star), P(5-star)]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PREDICTION                                   â”‚
â”‚                                                                      â”‚
â”‚            Predicted Star Rating (1-5)                               â”‚
â”‚                                                                      â”‚
â”‚  Example Output: [0.05, 0.08, 0.12, 0.25, 0.50]                     â”‚
â”‚                   â†“                                                  â”‚
â”‚            Predicted Rating: 5-star (argmax)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Layer Specifications

### Layer-by-Layer Breakdown

| Layer # | Layer Type | Input Shape | Output Shape | Parameters | Activation | Notes |
|---------|------------|-------------|--------------|------------|------------|-------|
| 1 | Embedding | (None, 37) | (None, 37, 128) | 1,280,000 | None | Learnable word vectors |
| 2 | Bidirectional(GRU) | (None, 37, 128) | (None, 64) | 110,208 | tanh | Forward + Backward GRU |
| 3 | Dropout | (None, 64) | (None, 64) | 0 | None | Rate = 0.5 |
| 4 | Dense | (None, 64) | (None, 64) | 4,160 | ReLU | L2 regularization |
| 5 | Dropout | (None, 64) | (None, 64) | 0 | None | Rate = 0.5 |
| 6 | Dense (Output) | (None, 64) | (None, 5) | 325 | Softmax | Classification layer |

**Total Parameters**: 1,394,693  
**Trainable Parameters**: 1,394,693  
**Non-trainable Parameters**: 0

---

## Training Configuration

### Optimizer
```
Algorithm: Adam (Adaptive Moment Estimation)
Learning Rate: 5e-5 (0.00005)
Beta_1: 0.9 (default)
Beta_2: 0.999 (default)
Epsilon: 1e-7 (default)
```

### Loss Function
```
Categorical Cross-Entropy
Formula: -Î£(y_true * log(y_pred))
```

### Callbacks
```
1. EarlyStopping
   - Monitor: validation loss
   - Patience: 2 epochs
   - Restore best weights: Yes

2. ReduceLROnPlateau
   - Monitor: validation loss
   - Factor: 0.5 (halve learning rate)
   - Patience: 2 epochs
   - Min LR: 1e-6
```

### Training Hyperparameters
```
Batch Size: 128
Epochs: 10 (early stopping active)
Validation Split: 10% of training data
Class Weights: Computed to handle imbalance
Data Augmentation: Oversampling minority classes (2-star, 3-star)
```

---

## Data Flow Example

### Step-by-Step Processing

**Input Text:**
```
"Sobrang bilis ng delivery at maganda packaging, pero medyo mahal siya"
```

**After Preprocessing:**
```
"sobrang bilis delivery maganda packaging medyo mahal"
(stopwords removed, lowercased, cleaned)
```

**After Tokenization:**
```
[234, 567, 1234, 890, 445, 2341, 1567]
(word indices from vocabulary)
```

**After Padding (max_length=37):**
```
[234, 567, 1234, 890, 445, 2341, 1567, 0, 0, 0, ..., 0]
(padded with zeros to length 37)
```

**After Embedding:**
```
Shape: (1, 37, 128)
Dense 128-dimensional vectors for each word
```

**After Bidirectional GRU:**
```
Shape: (1, 64)
Context-aware representation combining forward and backward passes
```

**After Dense Layers:**
```
Shape: (1, 5)
Raw logits before softmax
```

**Output Probabilities:**
```
[0.03, 0.05, 0.12, 0.35, 0.45]
 1â˜…    2â˜…    3â˜…    4â˜…    5â˜…
```

**Final Prediction:** 5-star (highest probability)

---

## Model Complexity Analysis

### Parameter Count Breakdown

| Component | Parameters | Percentage |
|-----------|-----------|------------|
| Embedding Layer | 1,280,000 | 91.77% |
| BiGRU Layer | 110,208 | 7.90% |
| Dense Hidden | 4,160 | 0.30% |
| Dense Output | 325 | 0.02% |
| **Total** | **1,394,693** | **100%** |

### Memory Requirements
- **Model Size**: ~5.5 MB (float32 precision)
- **Training Memory**: ~2 GB (batch size 128)
- **Inference Memory**: ~50 MB per batch

### Computational Complexity
- **Training Time per Epoch**: ~40 seconds (CPU)
- **Inference Time**: ~150ms per review
- **FLOPs**: ~2.8M per forward pass

---

## Key Architectural Decisions

### 1. Why Bidirectional GRU?
- âœ… Captures context from both directions (past and future)
- âœ… More efficient than LSTM (fewer parameters)
- âœ… Handles sequences up to 37 tokens without vanishing gradients
- âœ… Suitable for sentiment analysis where context matters

### 2. Why Not LSTM?
- GRU has fewer parameters (simpler gating mechanism)
- Faster training with similar performance
- Less prone to overfitting on this dataset size

### 3. Why 10,000 Vocabulary Size?
- Captures diverse Taglish expressions
- Handles code-switching patterns
- Balances coverage vs. memory

### 4. Why 128 Embedding Dimensions?
- Sufficient to capture semantic relationships
- Lower than typical (300) to prevent overfitting
- Hyperparameter tuning showed best performance

### 5. Why 32 GRU Units?
- Sweet spot between model capacity and overfitting
- 64 units showed marginal improvement with longer training
- 32 units sufficient for this task complexity

### 6. Why Heavy Dropout (0.5)?
- Dataset has 58K samples (moderate size)
- Prevents overfitting to majority class (5-star)
- Regularizes without hurting performance

### 7. Why No Attention Mechanism?
- Simpler architecture, easier to train
- BiGRU already captures important context
- Future enhancement opportunity

---

## Comparison with Alternative Architectures

| Architecture | Parameters | Test Acc | Training Time | Pros | Cons |
|--------------|-----------|----------|---------------|------|------|
| **BiGRU (Ours)** | 1.39M | 65.12% | 6 min | Balanced, efficient | Middle-class confusion |
| Unidirectional GRU | 0.70M | 61.5% | 4 min | Faster | Misses future context |
| BiLSTM | 1.85M | 64.8% | 8 min | Slightly better memory | Slower, more complex |
| CNN + GRU | 2.1M | 63.2% | 7 min | Good for n-grams | More hyperparameters |
| Vanilla RNN | 0.45M | 58.1% | 5 min | Simple | Vanishing gradients |

---

## Regularization Strategy

### Techniques Applied

1. **Dropout (0.5)**: Randomly drops 50% of neurons during training
2. **Recurrent Dropout (0.3)**: Applies dropout to GRU recurrent connections
3. **L2 Regularization (1e-4)**: Penalizes large weights in dense layer
4. **Early Stopping**: Stops training when validation loss plateaus
5. **Learning Rate Scheduling**: Reduces LR when stuck in plateau
6. **Data Augmentation**: Oversampling minority classes

### Impact on Performance
- Prevents overfitting (train/val accuracy gap < 5%)
- Improves generalization to test set
- Reduces sensitivity to initialization

---

## Future Architecture Enhancements

### Potential Improvements

1. **Add Attention Layer**
   - Focus on sentiment-bearing words
   - Improve interpretability
   - Expected: +2-3% accuracy

2. **Multi-Head Self-Attention**
   - Replace GRU with Transformer-style attention
   - Better long-range dependencies
   - Expected: +3-5% accuracy, but slower

3. **Hybrid CNN-GRU**
   - CNN for n-gram features
   - GRU for sequential context
   - Expected: +1-2% accuracy

4. **Ensemble Models**
   - Combine multiple architectures
   - Reduces variance
   - Expected: +2-4% accuracy

5. **Character-Level Embeddings**
   - Handle out-of-vocabulary words better
   - Capture morphology
   - Expected: Better robustness

---

## Visual Architecture Summary

```
ğŸ“Š MODEL STATISTICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Layers:        6
Total Parameters:    1,394,693
Trainable Params:    1,394,693
Model Size:          ~5.5 MB
Training Time:       ~6 minutes
Inference Time:      ~150ms/review
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¯ PERFORMANCE METRICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Test Accuracy:       65.12%
Macro F1-Score:      0.3954
5-Star Precision:    90.0%
1-Star Recall:       83.3%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš™ï¸ HYPERPARAMETERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Vocab Size:          10,000
Embedding Dim:       128
GRU Units:           32 (64 bidirectional)
Dropout:             0.5
Learning Rate:       5e-5
Batch Size:          128
Max Sequence Length: 37
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

**End of Architecture Documentation**
