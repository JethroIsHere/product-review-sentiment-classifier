{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1b995c",
   "metadata": {},
   "source": [
    "# TaglishProcessor\n",
    "\n",
    "# This notebook implements a Taglish-safe preprocessing class using TensorFlow Tokenizer\n",
    "# and scikit-learn for splitting data. It preserves punctuation and Tagalog stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaeecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Install & import libraries\n",
    "\n",
    "# Run this cell in a notebook to install dependencies if they are missing.\n",
    "# On Windows PowerShell, run: `pip install -r requirements.txt` (see project README)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaglishProcessor:\n",
    "    def __init__(self, vocab_size=5000, max_length=100, oov_token='<OOV>'):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.oov_token = oov_token\n",
    "        self.tokenizer = None\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Keep punctuation and stopwords (no removal)\n",
    "        return text\n",
    "\n",
    "    def fit_tokenizer(self, texts):\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=self.oov_token)\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def texts_to_padded_sequences(self, texts):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError('Tokenizer not fitted. Call fit_tokenizer first or use load_and_prep_data.')\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "        return padded\n",
    "\n",
    "    def load_and_prep_data(self, filepath, text_col='review_text', label_col='label', test_size=0.15, val_size=0.15, random_state=42):\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Clean texts\n",
    "        df[text_col] = df[text_col].astype(str).map(self.clean_text)\n",
    "\n",
    "        X = df[text_col].tolist()\n",
    "        y = df[label_col].astype(int).values\n",
    "\n",
    "        # Fit tokenizer on full data\n",
    "        self.fit_tokenizer(X)\n",
    "        X_padded = self.texts_to_padded_sequences(X)\n",
    "\n",
    "        # First split off test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(X_padded, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "        # Now split remaining into train and val. Compute val proportion relative to the temp set.\n",
    "        val_relative = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_relative, random_state=random_state, stratify=y_temp)\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test, self.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bfa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and set `path` to your CSV to run):\n",
    "# processor = TaglishProcessor(vocab_size=5000, max_length=100)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test, tokenizer = processor.load_and_prep_data('data/taglish_reviews.csv')\n",
    "# print('Shapes:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
