{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Install & import libraries\n",
    "\n",
    "# Run this cell in a notebook to install dependencies if they are missing.\n",
    "# On Windows PowerShell, run: `pip install -r requirements.txt` (see project README)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaglishProcessor:\n",
    "    def __init__(self, vocab_size=5000, max_length=100, oov_token='<OOV>', num_classes=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.oov_token = oov_token\n",
    "        self.num_classes = num_classes\n",
    "        self.tokenizer = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_label_to_int(label):\n",
    "        # Accept numeric labels (0/1/2 or 1-5 ratings) or text labels\n",
    "        if pd.isna(label):\n",
    "            return 1  # default to Neutral\n",
    "        try:\n",
    "            v = int(label)\n",
    "            # If rating 1-5 map: 4-5 -> Good(2), 3 -> Neutral(1), 1-2 -> Bad(0)\n",
    "            if v in (0, 1, 2):\n",
    "                return int(v)\n",
    "            if 1 <= v <= 5:\n",
    "                if v >= 4:\n",
    "                    return 2\n",
    "                if v == 3:\n",
    "                    return 1\n",
    "                return 0\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            s = str(label).strip().lower()\n",
    "            if s in ('good', 'positive', 'pos', 'ganda', 'maganda'):\n",
    "                return 2\n",
    "            if s in ('neutral', 'neutrality', 'n') or s == 'okay' or s == 'ok':\n",
    "                return 1\n",
    "            if s in ('bad', 'negative', 'neg', 'sira', 'hindi maganda'):\n",
    "                return 0\n",
    "            # fallback: if phrase contains positive/negative words\n",
    "            if 'maganda' in s or 'good' in s or 'love' in s or 'ganda' in s:\n",
    "                return 2\n",
    "            if 'sira' in s or 'di' in s or 'hindi' in s or 'bad' in s:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Lowercase\n",
    "        text = str(text).lower()\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Keep punctuation and stopwords (no removal)\n",
    "        return text\n",
    "\n",
    "    def fit_tokenizer(self, texts):\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=self.oov_token)\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def texts_to_padded_sequences(self, texts):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError('Tokenizer not fitted. Call fit_tokenizer first or use load_and_prep_data.')\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "        return padded\n",
    "\n",
    "    def load_and_prep_data(self, filepath, text_col='review_text', label_col='label', test_size=0.15, val_size=0.15, random_state=42):\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Clean texts\n",
    "        df[text_col] = df[text_col].astype(str).map(self.clean_text)\n",
    "\n",
    "        X = df[text_col].tolist()\n",
    "        # Map labels to integers 0=Bad,1=Neutral,2=Good\n",
    "        y_int = df[label_col].map(self._map_label_to_int).astype(int).values\n",
    "\n",
    "        # Fit tokenizer on full data\n",
    "        self.fit_tokenizer(X)\n",
    "        X_padded = self.texts_to_padded_sequences(X)\n",
    "\n",
    "        # First split off test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(X_padded, y_int, test_size=test_size, random_state=random_state, stratify=y_int)\n",
    "        # Now split remaining into train and val. Compute val proportion relative to the temp set.\n",
    "        val_relative = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train_int, y_val_int = train_test_split(X_temp, y_temp, test_size=val_relative, random_state=random_state, stratify=y_temp)\n",
    "\n",
    "        # One-hot encode labels for training\n",
    "        y_train_cat = to_categorical(y_train_int, num_classes=self.num_classes)\n",
    "        y_val_cat = to_categorical(y_val_int, num_classes=self.num_classes)\n",
    "        y_test_cat = to_categorical(y_test, num_classes=self.num_classes)\n",
    "\n",
    "        # Return both categorical (for training) and integer labels (for evaluation/prediction)\n",
    "        return X_train, X_val, X_test, y_train_cat, y_val_cat, y_test_cat, y_train_int, y_val_int, y_test, self.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8270437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and set `path` to your CSV to run):\n",
    "# processor = TaglishProcessor(vocab_size=5000, max_length=100)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test, tokenizer = processor.load_and_prep_data('data/taglish_reviews.csv')\n",
    "# print('Shapes:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b10fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Dataset Loader (Option A)\n",
    "def load_hf_dataset_to_csv(dataset_id, save_path=r'..\\data\\hf_dataset.csv', prefer_split='train'):\n",
    "    \"\"\"Load a Hugging Face dataset by id and save a CSV with detected text/label columns.\n",
    "    Replace `dataset_id` with the dataset's repo id from https://huggingface.co/datasets.\"\"\"\n",
    "    try:\n",
    "        # Lazy import; install with `pip install datasets` if missing\n",
    "        from datasets import load_dataset\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('Please install the `datasets` package: pip install datasets') from e\n",
    "    ds = load_dataset(dataset_id)\n",
    "    # If dataset has multiple splits, pick the preferred split if available\n",
    "    if isinstance(ds, dict):\n",
    "        split = prefer_split if prefer_split in ds else list(ds.keys())[0]\n",
    "        df = ds[split].to_pandas()\n",
    "    else:\n",
    "        df = ds.to_pandas()\n",
    "    # Heuristic for text column\n",
    "    text_col_candidates = ['review_text','review','text','content','body']\n",
    "    label_col_candidates = ['label','labels','sentiment','rating','stars']\n",
    "    text_col = next((c for c in text_col_candidates if c in df.columns), df.columns[0])\n",
    "    label_col = next((c for c in label_col_candidates if c in df.columns), None)\n",
    "    # Save a reduced CSV if label found, otherwise save text-only CSV\n",
    "    out_df = df[[text_col]] if label_col is None else df[[text_col, label_col]]\n",
    "    out_path = save_path\n",
    "    # Ensure parent dir exists\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(f'Saved dataset to {out_path}\\nDetected text column: {text_col}\\nDetected label column: {label_col}')\n",
    "    return out_path, text_col, label_col\n",
    "\n",
    "# Example (replace the dataset id below with a real HF dataset id):\n",
    "# load_hf_dataset_to_csv('some-user/electronics-reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39dec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SentiShop sqlite tables to CSV and preview marc3ee datasets\n",
    "import sqlite3, pandas as pd, os\n",
    "db_path = 'data/sentishop_db.sqlite3'\n",
    "if os.path.exists(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)['name'].tolist()\n",
    "    os.makedirs('data/sentishop_tables', exist_ok=True)\n",
    "    print('Found tables:', tables)\n",
    "    for t in tables:\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM '{t}'\", conn)\n",
    "            out = f'data/sentishop_tables/{t}.csv'\n",
    "            df.to_csv(out, index=False)\n",
    "            print(f'Exported {t} -> {out} (rows={len(df)})')\n",
    "        except Exception as e:\n",
    "            print('Skipping table', t, 'error:', e)\n",
    "    conn.close()\n",
    "else:\n",
    "    print('Database not found at', db_path)\n",
    "\n",
    "# Preview marc3ee CSVs (first 5 rows) and report shape\n",
    "for f in ['data/marc3ee_reviews.csv','data/marc3ee_datasetb_withpredictions.csv']:\n",
    "    if os.path.exists(f):\n",
    "        df_preview = pd.read_csv(f, nrows=5)\n",
    "        try:\n",
    "            total_rows = sum(1 for _ in open(f)) - 1\n",
    "        except Exception:\n",
    "            total_rows = 'unknown'\n",
    "        print(f'\\nPreview of {f} (approx rows: {total_rows}):')\n",
    "        display(df_preview)\n",
    "    else:\n",
    "        print('File not found:', f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
