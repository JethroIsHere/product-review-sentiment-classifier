{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Install & import libraries\n",
    "\n",
    "# Run this cell in a notebook to install dependencies if they are missing.\n",
    "# On Windows PowerShell, run: `pip install -r requirements.txt` (see project README)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaglishProcessor:\n",
    "    def __init__(self, vocab_size=5000, max_length=100, oov_token='<OOV>', num_classes=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.oov_token = oov_token\n",
    "        self.num_classes = num_classes\n",
    "        self.tokenizer = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_label_to_int(label):\n",
    "        # Accept numeric labels (0/1/2 or 1-5 ratings) or text labels\n",
    "        if pd.isna(label):\n",
    "            return 1  # default to Neutral\n",
    "        try:\n",
    "            v = int(label)\n",
    "            # If rating 1-5 map: 4-5 -> Good(2), 3 -> Neutral(1), 1-2 -> Bad(0)\n",
    "            if v in (0, 1, 2):\n",
    "                return int(v)\n",
    "            if 1 <= v <= 5:\n",
    "                if v >= 4:\n",
    "                    return 2\n",
    "                if v == 3:\n",
    "                    return 1\n",
    "                return 0\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            s = str(label).strip().lower()\n",
    "            if s in ('good', 'positive', 'pos', 'ganda', 'maganda'):\n",
    "                return 2\n",
    "            if s in ('neutral', 'neutrality', 'n') or s == 'okay' or s == 'ok':\n",
    "                return 1\n",
    "            if s in ('bad', 'negative', 'neg', 'sira', 'hindi maganda'):\n",
    "                return 0\n",
    "            # fallback: if phrase contains positive/negative words\n",
    "            if 'maganda' in s or 'good' in s or 'love' in s or 'ganda' in s:\n",
    "                return 2\n",
    "            if 'sira' in s or 'di' in s or 'hindi' in s or 'bad' in s:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Lowercase\n",
    "        text = str(text).lower()\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Keep punctuation and stopwords (no removal)\n",
    "        return text\n",
    "\n",
    "    def fit_tokenizer(self, texts):\n",
    "        self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=self.oov_token)\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def texts_to_padded_sequences(self, texts):\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError('Tokenizer not fitted. Call fit_tokenizer first or use load_and_prep_data.')\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "        return padded\n",
    "\n",
    "    def load_and_prep_data(self, filepath, text_col='review_text', label_col='label', test_size=0.15, val_size=0.15, random_state=42):\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Clean texts\n",
    "        df[text_col] = df[text_col].astype(str).map(self.clean_text)\n",
    "\n",
    "        X = df[text_col].tolist()\n",
    "        # Map labels to integers 0=Bad,1=Neutral,2=Good\n",
    "        y_int = df[label_col].map(self._map_label_to_int).astype(int).values\n",
    "\n",
    "        # Fit tokenizer on full data\n",
    "        self.fit_tokenizer(X)\n",
    "        X_padded = self.texts_to_padded_sequences(X)\n",
    "\n",
    "        # First split off test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(X_padded, y_int, test_size=test_size, random_state=random_state, stratify=y_int)\n",
    "        # Now split remaining into train and val. Compute val proportion relative to the temp set.\n",
    "        val_relative = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train_int, y_val_int = train_test_split(X_temp, y_temp, test_size=val_relative, random_state=random_state, stratify=y_temp)\n",
    "\n",
    "        # One-hot encode labels for training\n",
    "        y_train_cat = to_categorical(y_train_int, num_classes=self.num_classes)\n",
    "        y_val_cat = to_categorical(y_val_int, num_classes=self.num_classes)\n",
    "        y_test_cat = to_categorical(y_test, num_classes=self.num_classes)\n",
    "\n",
    "        # Return both categorical (for training) and integer labels (for evaluation/prediction)\n",
    "        return X_train, X_val, X_test, y_train_cat, y_val_cat, y_test_cat, y_train_int, y_val_int, y_test, self.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8270437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and set `path` to your CSV to run):\n",
    "# processor = TaglishProcessor(vocab_size=5000, max_length=100)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test, tokenizer = processor.load_and_prep_data('data/taglish_reviews.csv')\n",
    "# print('Shapes:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
