{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b66655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam  # type: ignore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint  # type: ignore\n",
    "\n",
    "# Hyperparameters (defaults from project guide)\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_LENGTH = 100\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# Simple helper to log to CSV\n",
    "LOG_CSV = os.path.join('..', 'logs', 'hyperparameter_logs.csv')\n",
    "\n",
    "def log_experiment(params, test_accuracy):\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    row = {\n",
    "        'vocab_size': params['vocab_size'],\n",
    "        'max_length': params['max_length'],\n",
    "        'embedding_dim': params['embedding_dim'],\n",
    "        'lstm_units': params['lstm_units'],\n",
    "        'dropout': params['dropout'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'epochs': params['epochs'],\n",
    "        'num_classes': params.get('num_classes', NUM_CLASSES),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    df = pd.DataFrame([row])\n",
    "    if os.path.exists(LOG_CSV):\n",
    "        df.to_csv(LOG_CSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(LOG_CSV, index=False)\n",
    "\n",
    "# NOTE: This notebook expects you to prepare data with `data_processor.ipynb` or to provide a CSV path below.\n",
    "# For convenience, the TaglishProcessor class is available in `data_processor.ipynb`. If you prefer, copy the class\n",
    "# into this notebook or run the processor notebook first and save `tokenizer` + arrays to disk.\n",
    "\n",
    "print('Open `data_processor.ipynb` to prepare data, then run training cells here.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
