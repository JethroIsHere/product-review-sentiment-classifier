================================================================================
CCS 248 FINAL PROJECT MASTER GUIDE
Project Title: Taglish Product Review Sentiment Classifier

PART 1: PROJECT PROPOSAL & DESIGN
(Use this section for your documentation/report)

PROBLEM STATEMENT
For online electronics purchases in the Philippines, customers rely heavily on user reviews to make informed decisions. However, a significant portion of these reviews are written in "Taglish" (a mix of Tagalog and English).

Standard sentiment analysis models trained on pure English datasets fail to capture the nuance of Taglish (e.g., "Maganda sana kaso sira," where context flips at the end). Manually reading thousands of reviews is impractical.

This project automates this by training a specific Neural Network to classify Taglish reviews as "Good" (Positive) or "Bad" (Negative), helping local consumers and sellers understand feedback at scale.

TARGET PRODUCT CATEGORY
This project focuses on consumer electronics product reviews (mobile phones, chargers, headphones, and related accessories). Restricting the dataset to electronics helps target domain-specific vocabulary (e.g., "battery", "charger", "screen") and improves dataset relevance for experiments and evaluation.

THE SOLUTION (Model Architecture)
We will use a Recurrent Neural Network (RNN), specifically a Long Short-Term Memory (LSTM) network, trained from scratch.

Why LSTM? Unlike standard feed-forward networks, LSTMs have "memory." They process text as a sequence, allowing the model to understand context. For example, in the phrase "Hindi maganda" (Not beautiful), the model remembers "Hindi" to negate "maganda."

Why From Scratch? To strictly adhere to the course requirement prohibiting pre-trained transformers (like BERT), we will build the Embedding Layer and LSTM layers manually using TensorFlow/Keras.

DATASET STRATEGY

Source: Lazada or Shopee Philippines Product Reviews (available via Kaggle).

Nature: Real-world, noisy Taglish text.

Data Split:

70% Training (Teaching the model)

15% Validation (Tuning hyperparameters)

15% Testing (Final evaluation for grading)

TOOLS & LIBRARIES

Language: Python

Framework: TensorFlow / Keras (for the Neural Network)

Data Manipulation: Pandas, NumPy

Preprocessing: Scikit-learn (for splitting data)

PART 2: IMPLEMENTATION GUIDE (GITHUB COPILOT PROMPTS)
(Copy/Paste these prompts into Copilot to generate your code)

FILE 1: DEPENDENCIES
Filename: requirements.txt
[COPILOT PROMPT]:
"Create a requirements.txt file for a machine learning project using TensorFlow, Pandas, NumPy, Scikit-learn, and Matplotlib."

FILE 2: THE "TAGLISH" DATA PROCESSOR
Filename: data_processor.py
[CONTEXT]:
We need to handle Tagalog-English code-switching. Standard English cleaners often remove stopwords like "ang" or "hindi" which are critical for meaning in Tagalog.
[COPILOT PROMPT]:
"Write a Python class named TaglishProcessor using TensorFlow Tokenizer and Pad Sequences.

Include a clean_text method that converts text to lowercase and removes HTML tags, but KEEPS punctuation and stopwords (do not remove Tagalog stopwords).

Include a load_and_prep_data method that takes a filepath, loads a CSV, cleans the 'review_text' column, and tokenizes it.

It should split the data into Training, Validation, and Test sets (70/15/15 split).

Return the padded sequences and labels."

FILE 3: THE MODEL ARCHITECTURE (Rules: No BERT, Train from Scratch)
Filename: model_builder.py
[CONTEXT]:
The rubric explicitly bans pre-trained models. We must use a standard Keras Embedding layer.
[COPILOT PROMPT]:
"Create a function build_lstm_model using TensorFlow Keras Sequential API.
The function should accept arguments: vocab_size, embedding_dim, max_length, lstm_units, and dropout_rate.
The architecture must have:

An Embedding layer (trained from scratch).

A Bidirectional LSTM layer (to read text forwards and backwards).

A Dropout layer (0.5) to prevent overfitting.

A Dense output layer with sigmoid activation for binary classification."

FILE 4: THE TRAINING LOOP (With Rubric Logging)
Filename: train_runner.py
[CONTEXT]:
The rubric requires you to document every hyperparameter change. This script should automate that log.
[COPILOT PROMPT]:
"Write a main training script that uses TaglishProcessor and build_lstm_model.

Define constants at the top for hyperparameters: VOCAB_SIZE=5000, MAX_LENGTH=100, EMBEDDING_DIM=64, LSTM_UNITS=64, DROPOUT=0.5, LEARNING_RATE=0.001.

Load the data using the processor.

Compile the model using Adam optimizer with the defined learning rate.

Train the model for 5 epochs using the validation set.

Evaluate the model on the test set.

Crucial: Append the hyperparameters and the final Test Accuracy to a CSV file named 'hyperparameter_logs.csv' so I can track my experiments."

FILE 5: THE DEMO SCRIPT
Filename: predict.py
[CONTEXT]:
Use this to show your professor the model working in real-time.
[COPILOT PROMPT]:
"Write a script to load the saved Keras model and the tokenizer.
Create a function predict_sentiment that accepts a text string, preprocesses it using the same logic as training, and prints whether the sentiment is 'Positive' or 'Negative' based on a 0.5 threshold.
Add a while-loop at the end to let me type reviews interactively in the terminal."

PART 3: RUBRIC CHECKLIST & EXPERIMENTS
(Do these to ensure full points)

PROBLEM SELECTION (15pts):

Justification: "Taglish is complex and unsupported by standard models."

DATASET CHOICE (15pts):

Action: Download a Lazada/Shopee dataset from Kaggle.

Validation: Ensure you have at least 3,000 reviews.

ARCHITECTURE SELECTION (20pts):

Design: You are using LSTM.

Explanation: "LSTM was chosen over CNN because text is sequential, and sentiment often depends on word order."

MODEL TRAINING & TUNING (25pts):

You need to run train_runner.py at least 3 times with different settings.

Experiment A: Run with LSTM_UNITS = 64. Check accuracy.

Experiment B: Change to LSTM_UNITS = 32. Check accuracy.

Experiment C: Change DROPOUT = 0.2. Check accuracy.

The script will auto-save these results to hyperparameter_logs.csv. Submit this CSV.

CODE SUBMISSION (5pts):

Push all these files to the GitHub repository your professor provides.
